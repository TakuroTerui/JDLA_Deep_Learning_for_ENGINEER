{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "全結合型ニューラルネットワーク.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDa/rLcB4qKT14Jl46QQWI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4LxEFqTv1VqZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(X):\n",
        "    return np.maxmum(0, X)\n",
        "\n",
        "def softmax(X):\n",
        "    X = X - np.max(X, axis=1, keepdims=True)\n",
        "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
        "\n",
        "def relu_backward(Z, delta):\n",
        "    delta[Z == 0] = 0\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(t * np.log(y + 1e-7) / batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedNeuralNetwork():\n",
        "    def __init__(self, layer_units):\n",
        "        '''\n",
        "        layer_units: list, 各層のノード数を格納したリスト\n",
        "        '''\n",
        "        self.n_iter = 0\n",
        "        self.t_ = 0\n",
        "        self.layer_units = layer_units\n",
        "        self.n_layers_ = len(layer_units)\n",
        "\n",
        "        # パラメータの初期化\n",
        "        self.coefs_ = []\n",
        "        self.intercepts_ = []\n",
        "        for i in range(self.n_layers_ - 1):\n",
        "            # coef_init, intercept_init = self._init_coef(( ア ))\n",
        "            coef_init, intercept_init = self._init_coef(layer_units[i], layer_units[i + 1])\n",
        "            self.coefs_.append(coef_init)\n",
        "            self.intercepts_.append(intercept_init)\n",
        "        \n",
        "        # 勾配の初期化\n",
        "        self.coef_grads_ = [np.empty((n_in_, n_out_)) for n_in_, n_out_ in zip(layer_units[:-1], layer_units[1:])]\n",
        "        self.intercept_grads_ = [np.empty(n_out_) for n_out_ in layer_units[1:]]\n",
        "    \n",
        "    def _init_coef(self, n_in, n_out):\n",
        "        '''\n",
        "        ある層間のパラメータを初期化するメソッド\n",
        "        n_in: int, 入力側のノード数\n",
        "        n_out: int, 出力側のノード数\n",
        "        '''\n",
        "        std = np.sqrt(2 / n_in)\n",
        "        coef_init = np.random.randn(n_in, n_out) * std\n",
        "        intercept_init = np.zeros(n_out)\n",
        "        return coef_init, intercept_init\n",
        "    \n",
        "    def forward(self, activations):\n",
        "        '''\n",
        "        順伝播処理を行うメソッド\n",
        "        activartions: list, 各層の出力を納めたリスト\n",
        "                     activation[0]は入力データ\n",
        "                     activation[i].shape=(バッチサイズ、ノード数)\n",
        "        '''\n",
        "        affine = [None] * [self.n_layers_ - 1]\n",
        "        for i in range(self.n_layers_ - 1):\n",
        "            # アフィン変換\n",
        "            affine[i] = np.dot(activations[i], self.coefs_[i]) + self.intercepts_[i]\n",
        "\n",
        "            # if (i + 1) == (( イ )):\n",
        "            if (i + 1) == (self.n_layers_ - 1):\n",
        "                '''\n",
        "                出力層の場合\n",
        "                '''\n",
        "                activations[i + 1] = softmax(affine[i])\n",
        "            else:\n",
        "                '''\n",
        "                隠れ層の場合\n",
        "                '''\n",
        "                activations[i + 1] = relu(affine(i))\n",
        "        \n",
        "        return activations\n",
        "    \n",
        "    def _grad(self, j, activations, deltas):\n",
        "        '''\n",
        "        各パラメータの勾配を算出するメソッド\n",
        "        j: int, アフィンの番号\n",
        "        activations: list, 各層の出力を納めたメソッド\n",
        "        deltas: list, 出力層側から伝わってきた勾配を納めたリスト\n",
        "        '''\n",
        "        # self.coef_grads_[j] = ( ウ )\n",
        "        self.coef_grads_[j] = np.dot(activations[j].T, deltas[j])\n",
        "        # self.intercept_grads_[j] = ( エ )\n",
        "        self.intercept_grads_[j] = np.sum(deltas[j], axis=0)\n",
        "    \n",
        "    def _backward(self, t, activations):\n",
        "        '''\n",
        "        逆伝播処理を行うメソッド\n",
        "        t: array-like, 正解ラベル, t.shape=(バッチサイズ、出力層ノード数)\n",
        "        activations: list, 各層の出力を納めたリスト\n",
        "        '''\n",
        "        deltas = [None] * (self.n_layers_ - 1)\n",
        "        last = self.n_layers_ - 2\n",
        "    \n",
        "        # 交差エントロピー誤差とソフトマックス関数を合わせて勾配を算出\n",
        "        n_samples = t.shape[0]\n",
        "        # deltas[last] = ( オ )\n",
        "        deltas[last] = (activations[-1] - t) / n_samples\n",
        "\n",
        "        # 出力層の1つ手前のパラメータの勾配を算出\n",
        "        # self._grad(( カ ), activations, deltas)\n",
        "        self._grad(last, activations, deltas)\n",
        "\n",
        "        # 残りのパラメータの勾配を算出\n",
        "        for i in range(self.n_layers_ - 2, 0, -1):\n",
        "            # 入力(activations)の勾配を算出\n",
        "            # deltas[i - 1] = ( キ )\n",
        "            deltas[i - 1] = np.dot(deltas[i], self.coefs_[i].T)\n",
        "\n",
        "            # 活性化関数ReLuの勾配を算出\n",
        "            # relu_backward(( ク ), deltas[i - 1])\n",
        "            relu_backward(activations[i], deltas[i - 1])\n",
        "\n",
        "            # パラメータの勾配を算出\n",
        "            # self._grad(( ケ ), activations, deltas)\n",
        "            self._grad(i - 1, activations, deltas)\n",
        "        \n",
        "        return\n",
        "    \n",
        "    def _forward_and_backward(self, x, t):\n",
        "        '''\n",
        "        順伝播処理を実行した後、逆伝播処理を実行するメソッド\n",
        "        x: array-like, 入力データ, x.shape=(バッチサイズ、入力層ノード数)\n",
        "        t: array-like, 正解ラベル, t.shape=(バッチサイズ、出力層ノード数)\n",
        "        '''\n",
        "        activations = [x] + [None] * (self.n_layers_ - 1)\n",
        "\n",
        "        # 順伝播\n",
        "        activations = self._forward(activations)\n",
        "        loss = cross_entropy_error(activations[-1], t)\n",
        "\n",
        "        # 逆伝播\n",
        "        self._backward(t, activations)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "WMY_ZbPT2MMH"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}