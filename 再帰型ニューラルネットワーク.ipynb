{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "再帰型ニューラルネットワーク.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFsejtVQ2Ns7zoGQW8KFh8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyWOUXWVnUlG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, h0):\n",
        "        '''\n",
        "        Wx: 入力xにかかる重み（1、隠れ層のノード数）\n",
        "        Wh: 1時刻前のhにかかる重み（隠れ層のノード数、隠れ層のノード数）\n",
        "        h0: 隠れ層の初期値（データ数、隠れ層のノード数）\n",
        "        '''\n",
        "        # パラメータのリスト\n",
        "        self.params = [Wx, Wh]\n",
        "\n",
        "        # 隠れそうの初期値を設定\n",
        "        self.h_prev = h0\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        順伝播計算\n",
        "        x: 入力（データ数、1）\n",
        "        '''\n",
        "        Wx, Wh = self.params\n",
        "        h_prev = self.h_prev\n",
        "\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx)\n",
        "\n",
        "        # 活性化関数は恒等写像関数とする\n",
        "        h_next = t\n",
        "\n",
        "        # 隠れ層の状態の保存\n",
        "        self.h_prev = h_next\n",
        "\n",
        "        return h_next"
      ],
      "metadata": {
        "id": "Tp_ISK5NndMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "def forward(x, h_prev, c_prev, Wx, Wh, b):\n",
        "    '''\n",
        "    順伝播計算\n",
        "    x: 入力（データ数、特徴量の数）\n",
        "    h_prev: 前状態の隠れ状態の出力（データ数、隠れ層のノード数）\n",
        "    c_prev: 前状態のメモリの状態（データ数、隠れ層のノード数）\n",
        "    Wx: 入力x用の重みパラメータ（特徴量の数、4＊隠れ層のノード数）\n",
        "    Wh: 隠れ状態h用の重みパラメータ（隠れ層のノード数、4＊隠れ層のノード数）\n",
        "    b: バイアス（4＊隠れ層のノード数）\n",
        "    '''\n",
        "    N, H = h_prev.shape\n",
        "\n",
        "    A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
        "\n",
        "    f = A[:, :H]\n",
        "    g = A[:, H:2*H]\n",
        "    i = A[:, 2*H:3*H]\n",
        "    o = A[:, 3*H:]\n",
        "\n",
        "    f = sigmoid(f)\n",
        "    g = np.tanh(g)\n",
        "    i = sigmoid(i)\n",
        "    o = sigmoid(o)\n",
        "\n",
        "    print(f.shape, c_prev.shape, g.shape, i.shape)\n",
        "    c_next = f * c_prev + g * i\n",
        "    h_next = o * np.tanh(c_next)\n",
        "\n",
        "    return h_next, c_next"
      ],
      "metadata": {
        "id": "ZJ5XjbKMoxXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グローバル・アテンションモデル\n",
        "def forward(self, hs, h):\n",
        "    '''\n",
        "    順伝播\n",
        "    重みベクトルを求めるための関数\n",
        "    hs: エンコーダーにおける全ての隠れ状態（データ数、時刻数、隠れ層のノード数）\n",
        "    h: デコーダーにおけるある時刻の隠れ状態（データ数、隠れ層のノード数）\n",
        "    '''\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
        "\n",
        "    t = hs * hr\n",
        "    s = np.sum(t, axis=2)\n",
        "\n",
        "    # ソフトマックス関数に通すことで、正規化する\n",
        "    a = softmax(s)\n",
        "\n",
        "    return a"
      ],
      "metadata": {
        "id": "3HtfYaXKqNi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LeyQ-IkfrFX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}